{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvGIxp4eyALi"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8HXEZGtyIQh"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1',is_slippery=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAoTteifyI2c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_state_value(env, current_state, value_matrix, discount_factor):\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    action_values = np.zeros(shape=num_actions)\n",
        "    for action in range(num_actions):\n",
        "        for transition_prob, next_state, reward, done in env.env.P[current_state][action]:\n",
        "            action_values[action] += transition_prob * (reward + discount_factor * value_matrix[next_state])\n",
        "    return action_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EheuMWhxyJd0"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(policy_matrix, environment, discount_factor=1.0, convergence_threshold=1e-9, max_iterations=1000):\n",
        "    num_states = environment.observation_space.n\n",
        "    evaluation_iterations = 1\n",
        "    state_values = np.zeros(shape=num_states)\n",
        "\n",
        "    for iteration in range(int(max_iterations)):\n",
        "        delta = 0\n",
        "        for current_state in range(num_states):\n",
        "            new_state_value = 0\n",
        "            for action, action_probability in enumerate(policy_matrix[current_state]):\n",
        "                for state_probability, next_state, reward, done in environment.P[current_state][action]:\n",
        "                    new_state_value += action_probability * state_probability * (reward + discount_factor * state_values[next_state])\n",
        "            delta = max(delta, np.abs(state_values[current_state] - new_state_value))\n",
        "            state_values[current_state] = new_state_value\n",
        "\n",
        "        evaluation_iterations += 1\n",
        "\n",
        "        if delta < convergence_threshold:\n",
        "            print(f'Policy evaluation terminated after {evaluation_iterations} iterations.\\n')\n",
        "            return state_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IryDdEYmDs8Y"
      },
      "outputs": [],
      "source": [
        "def policy_iteration_algorithm(environment, discount_factor=1.0, max_iterations=1000):\n",
        "    num_states = environment.observation_space.n\n",
        "    num_actions = environment.action_space.n\n",
        "    policy_matrix = np.ones(shape=[num_states, num_actions]) / num_actions\n",
        "    evaluated_policies_count = 1\n",
        "\n",
        "    for iteration in range(int(max_iterations)):\n",
        "        stable_policy = False\n",
        "        value_function = evaluate_policy(policy_matrix, environment, discount_factor)\n",
        "\n",
        "        for current_state in range(num_states):\n",
        "            current_action = np.argmax(policy_matrix[current_state])\n",
        "            action_values = calculate_state_value(environment, current_state, value_function, discount_factor)\n",
        "            best_action = np.argmax(action_values)\n",
        "\n",
        "            if current_action != best_action:\n",
        "                stable_policy = True\n",
        "\n",
        "            policy_matrix[current_state] = np.eye(num_actions)[best_action]\n",
        "\n",
        "        evaluated_policies_count += 1\n",
        "\n",
        "        if stable_policy:\n",
        "            print(f'Found a stable policy after {evaluated_policies_count:,} evaluations.\\n')\n",
        "            return policy_matrix, value_function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2-KjjYmyKSM"
      },
      "outputs": [],
      "source": [
        "def value_iteration_algorithm(environment, discount_factor=1e-1, convergence_threshold=1e-9, max_iterations=1e4):\n",
        "    state_values = np.zeros(environment.observation_space.n)\n",
        "\n",
        "    for iteration in range(int(max_iterations)):\n",
        "        delta = 0\n",
        "\n",
        "        for current_state in range(environment.observation_space.n):\n",
        "            action_values = calculate_state_value(environment, current_state, state_values, discount_factor)\n",
        "            best_action_value = np.max(action_values)\n",
        "            delta = max(delta, np.abs(state_values[current_state] - best_action_value))\n",
        "            state_values[current_state] = best_action_value\n",
        "\n",
        "        if delta < convergence_threshold:\n",
        "            print(f'\\nValue iteration converged at iteration #{iteration+1:,}')\n",
        "            break\n",
        "\n",
        "    policy_matrix = np.zeros(shape=[environment.observation_space.n, environment.action_space.n])\n",
        "\n",
        "    for current_state in range(environment.observation_space.n):\n",
        "        action_values = calculate_state_value(environment, current_state, state_values, discount_factor)\n",
        "        best_action = np.argmax(action_values)\n",
        "        policy_matrix[current_state, best_action] = 1.0\n",
        "\n",
        "    return policy_matrix, state_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HA0k2Qh8yK7R"
      },
      "outputs": [],
      "source": [
        "def play_episodes_and_evaluate(env, num_episodes, policy_matrix, max_actions=100, render=False):\n",
        "\n",
        "    total_wins = 0\n",
        "    total_rewards, total_actions = 0, 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        current_state = env.reset()\n",
        "        episode_done, actions_taken = False, 0\n",
        "\n",
        "        while actions_taken < max_actions:\n",
        "            selected_action = np.argmax(policy_matrix[current_state])\n",
        "            next_state, reward, episode_done, _ = env.step(selected_action)\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            actions_taken += 1\n",
        "            total_rewards += reward\n",
        "            current_state = next_state\n",
        "\n",
        "            if episode_done:\n",
        "                total_wins += 1\n",
        "                break\n",
        "\n",
        "        total_actions += actions_taken\n",
        "\n",
        "    print(f'Total rewards: {total_rewards:,}\\tMax actions: {actions_taken:,}')\n",
        "\n",
        "    average_reward = total_rewards / num_episodes\n",
        "    average_actions = total_actions / num_episodes\n",
        "\n",
        "    print('')\n",
        "    return total_wins, total_rewards, average_reward, average_actions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo2MuWYQFnAW"
      },
      "outputs": [],
      "source": [
        "num_episodes = 1000\n",
        "\n",
        "def agent_and_evaluate(env):\n",
        "\n",
        "    total_rewards_list = []\n",
        "\n",
        "    action_mapping = {\n",
        "        0: '\\u2191',  # up\n",
        "        1: '\\u2192',  # right\n",
        "        2: '\\u2193',  # down\n",
        "        3: '\\u2190'   # left\n",
        "    }\n",
        "\n",
        "    iteration_methods = [\n",
        "        ('Policy Iteration', policy_iteration_algorithm),\n",
        "        ('Value Iteration', value_iteration_algorithm)\n",
        "    ]\n",
        "\n",
        "    for method_name, method_func in iteration_methods:\n",
        "        policy_matrix, value_function = method_func(env)\n",
        "\n",
        "        print(f'Final policy using {method_name}:')\n",
        "        print(' '.join([action_mapping[action] for action in np.argmax(policy_matrix, axis=1)]))\n",
        "\n",
        "        total_wins, total_rewards, avg_reward, avg_actions = play_episodes_and_evaluate(env, num_episodes, policy_matrix)\n",
        "        total_rewards_list.append(total_rewards)\n",
        "\n",
        "        print(f'Number of wins = {total_wins:,}')\n",
        "        print(f'Average reward = {avg_reward:.2f}')\n",
        "        print(f'Average actions = {avg_actions:.2f}')\n",
        "\n",
        "    return total_rewards_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2UG0B9OGdjA",
        "outputId": "8eff5873-bad4-461d-bc8e-88b0fe5c1283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy evaluation terminated after 66 iterations.\n",
            "\n",
            "Found a stable policy after 2 evaluations.\n",
            "\n",
            "Final policy using Policy Iteration:\n",
            "↑ ← ↑ ← ↑ ↑ ↑ ↑ ← → ↑ ↑ ↑ ↓ → ↑\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rewards: 735.0\tMax actions: 45\n",
            "\n",
            "Number of wins = 1,000\n",
            "Average reward = 0.73\n",
            "Average actions = 41.14\n",
            "\n",
            "Value iteration converged at iteration #8\n",
            "Final policy using Value Iteration:\n",
            "→ ← ↓ ← ↑ ↑ ↑ ↑ ← → ↑ ↑ ↑ ↓ → ↑\n",
            "Total rewards: 438.0\tMax actions: 24\n",
            "\n",
            "Number of wins = 1,000\n",
            "Average reward = 0.44\n",
            "Average actions = 27.64\n"
          ]
        }
      ],
      "source": [
        "rewards = agent_and_evaluate(env)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
